<!DOCTYPE html>
<html>
<head>
  <link href='https://fonts.googleapis.com/css?family=Roboto:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Roboto+Mono:300' rel='stylesheet' type='text/css'>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
<link rel="stylesheet" href="https://skiadas.github.io/css/course.css" type="text/css" />

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>
<body>
<section id="advanced-lab-3-linear-modeling-and-anova" class="level1">
<h1>Advanced Lab 3: Linear Modeling and ANOVA</h1>
<section id="introduction" class="level2">
<h2>Introduction</h2>
<p>In this lab we will study key linear modeling techniques. We will also practice some data cleanup and import steps.</p>
<p>To begin with, our data is in an SPSS file, which we can access using the <code>haven</code> library. You will probably want to create a new empty project first. Then you should download <a href="../datasets/targeting.sav">this data file</a>, and upload it to your project directory. You can also find a link in the workshop resources page.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(hanoverbase)
<span class="kw">library</span>(haven)
targeting &lt;-<span class="st"> </span><span class="kw">read_sav</span>(<span class="st">&quot;targeting.sav&quot;</span>)
<span class="co"># View(targeting)</span></code></pre></div>
<p>The data set contains a number of factor variables which are currently coded into column names. There is there race of the target (White/Black), whether the target was armed or unarmed, and whether a correct or incorrect shot action was taken. We will need to create these factor variables as we process the columns.</p>
</section>
<section id="cleaning-up-the-dataset" class="level2">
<h2>Cleaning up the dataset</h2>
<p>Let’s take a look at the variables:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">names</span>(targeting)</code></pre></div>
<p>We will only need the first 12 variables, the remaining are computed quantities. We will use a <code>select</code> for that. Then we will <code>gather</code> the 8 columns that contain observations (columns 3 through 10). Don’t worry about the warning.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">targetingLong &lt;-<span class="st"> </span>targeting <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">select</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">12</span>) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">gather</span>(<span class="dt">key=</span><span class="st">&quot;condition&quot;</span>, <span class="dt">value=</span><span class="st">&quot;time&quot;</span>, <span class="dv">3</span><span class="op">:</span><span class="dv">10</span>)</code></pre></div>
<p>Next we need to break the <code>key</code> variable into two parts, one showing the target’s race and another showing the outcome. We’ll first mutate the <code>key</code> field to remove everything up through the underscore. We will need the <code>stringr</code> package for that. This package allows us to perform various string-related tasks.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(stringr)
targetingLong &lt;-<span class="st"> </span>targeting <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">select</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">12</span>) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">gather</span>(<span class="dt">key=</span><span class="st">&quot;condition&quot;</span>, <span class="dt">value=</span><span class="st">&quot;time&quot;</span>, <span class="dv">3</span><span class="op">:</span><span class="dv">10</span>) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">condition=</span><span class="kw">str_replace</span>(condition, <span class="st">&quot;expressions.MeanRT_&quot;</span>, <span class="st">&quot;&quot;</span>))</code></pre></div>
<p>Now we split the new key variable in two parts, splitting after the first 5 characters (to capture the White/Black part):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">targetingLong &lt;-<span class="st"> </span>targeting <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">select</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">12</span>) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">gather</span>(<span class="dt">key=</span><span class="st">&quot;condition&quot;</span>, <span class="dt">value=</span><span class="st">&quot;time&quot;</span>, <span class="dv">3</span><span class="op">:</span><span class="dv">10</span>) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">condition=</span><span class="kw">str_replace</span>(condition, <span class="st">&quot;expressions.MeanRT_&quot;</span>, <span class="st">&quot;&quot;</span>)) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">separate</span>(condition, <span class="dt">into=</span><span class="kw">c</span>(<span class="st">&quot;race&quot;</span>, <span class="st">&quot;outcome&quot;</span>), <span class="dt">sep=</span><span class="dv">5</span>)</code></pre></div>
<p>Next we need to work on the <code>outcome</code> variable:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">targetingLong <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(outcome)</code></pre></div>
<p>This variable actually contains two different pieces of information, whether the target was armed and whether the subject took the correct action:</p>
<table>
<tbody>
<tr class="odd">
<td style="text-align: right;">Hits</td>
<td style="text-align: left;">Target was armed, subject fired (Correct Action)</td>
</tr>
<tr class="even">
<td style="text-align: right;">Misses</td>
<td style="text-align: left;">Target was armed, subject did not fire (Incorrect Action)</td>
</tr>
<tr class="odd">
<td style="text-align: right;">CRs</td>
<td style="text-align: left;">Target was unarmed, subject did not fire (Correct Rejection)</td>
</tr>
<tr class="even">
<td style="text-align: right;">FAs</td>
<td style="text-align: left;">Target was unarmed, subject did fire (False Alarm)</td>
</tr>
</tbody>
</table>
<p>We will use <code>mutate</code> and <code>recode_factor</code> to create these new variables.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">targetingFinal &lt;-<span class="st"> </span>targetingLong <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">weapon=</span><span class="kw">recode_factor</span>(outcome, <span class="dt">Hits=</span><span class="st">&quot;Armed&quot;</span>, <span class="dt">Misses=</span><span class="st">&quot;Armed&quot;</span>,
                                         <span class="dt">CRs=</span><span class="st">&quot;Unarmed&quot;</span>, <span class="dt">FAs=</span><span class="st">&quot;Unarmed&quot;</span>),
           <span class="dt">action=</span><span class="kw">recode_factor</span>(outcome, <span class="dt">Hits=</span><span class="st">&quot;Correct&quot;</span>, <span class="dt">Misses=</span><span class="st">&quot;Incorrect&quot;</span>,
                                         <span class="dt">CRs=</span><span class="st">&quot;Correct&quot;</span>, <span class="dt">FAs=</span><span class="st">&quot;Incorrect&quot;</span>))</code></pre></div>
<p>To double-check that we did this correctly, we’ll create counts:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">targetingFinal <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">count</span>(race, outcome, weapon, action)</code></pre></div>
<p>We should see 49 cases for each of 8 different combinations of values, corresponding to our initial 49 data rows.</p>
<p>Finally, a couple more cleanup steps are in order before we move on:</p>
<ul>
<li>We will fix the names of some of the variables, using <code>rename</code>.</li>
<li>We will drop the <code>outcome</code> column as it is no longer needed, using <code>select</code>.</li>
<li>We will code the the <code>gender</code>, <code>race</code> and <code>age</code> variables as factors, using <code>mutate</code> and <code>factor</code> for that (we would use <code>recode_factor</code> if we wanted to change the names of the labels, but we don’t).</li>
<li>We will remove the missing values from the <code>time</code> variable, using <code>filter</code>. The expression <code>!is.na(time)</code> picks up all those values that are <em>not</em> missing.</li>
</ul>
<p>This can all be done in a series of pipelined steps.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">targetingFinal &lt;-<span class="st"> </span>targetingFinal <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">rename</span>(<span class="dt">subject=</span><span class="st">&quot;script.subjectid&quot;</span>,
           <span class="dt">iat=</span><span class="st">&quot;expressions.d&quot;</span>,
           <span class="dt">gender=</span><span class="st">&quot;gender_response&quot;</span>,
           <span class="dt">age=</span><span class="st">&quot;age_response&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">select</span>(<span class="op">-</span>outcome) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">gender=</span><span class="kw">factor</span>(gender), <span class="dt">age=</span><span class="kw">ordered</span>(age), <span class="dt">race=</span><span class="kw">factor</span>(race)) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">filter</span>(<span class="op">!</span><span class="kw">is.na</span>(time))</code></pre></div>
<p>Let us visualize this data for a second. There are of course numerous plots we could do, but here is one possibility:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(targetingFinal) <span class="op">+</span><span class="st"> </span><span class="kw">aes</span>(<span class="dt">x=</span>action, <span class="dt">y=</span>time, <span class="dt">color=</span>race) <span class="op">+</span><span class="st"> </span><span class="kw">facet_grid</span>(<span class="op">~</span>weapon) <span class="op">+</span><span class="st"> </span><span class="kw">geom_boxplot</span>()</code></pre></div>
<p>We would like to see if there is a relation between reaction time when facing white targets and reaction time when facing black targets. In order to do that, we need to do the opposite of <code>gather</code>, which is <code>spread</code>. The idea is that we could spread the time values across two columns, one for the case of white targets and one for the case of black targets.For use of analysis in the future steps, we will exclude the rows that did not have values for both white and black average response time.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">targetingRaceDiff &lt;-<span class="st"> </span>targetingFinal <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">spread</span>(<span class="dt">key=</span>race, <span class="dt">value=</span>time) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">filter</span>(<span class="op">!</span><span class="kw">is.na</span>(White) <span class="op">&amp;</span><span class="st"> </span><span class="op">!</span><span class="kw">is.na</span>(Black))
targetingRaceDiff      <span class="co"># You can also use View</span></code></pre></div>
<p>Let’s do a simple scatterplot first:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(targetingRaceDiff) <span class="op">+</span>
<span class="st">    </span><span class="kw">aes</span>(<span class="dt">x=</span>White, <span class="dt">y=</span>Black, <span class="dt">color=</span>action) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_smooth</span>()</code></pre></div>
<p>And a more advanced scatterplot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(targetingRaceDiff) <span class="op">+</span>
<span class="st">    </span><span class="kw">aes</span>(<span class="dt">x=</span>White, <span class="dt">y=</span>Black) <span class="op">+</span>
<span class="st">    </span><span class="kw">facet_grid</span>(weapon<span class="op">~</span>action, <span class="dt">scales=</span><span class="st">&quot;free&quot;</span>) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_point</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_smooth</span>()</code></pre></div>
<p>In the subsequent sections, we will consider models that describe the <code>Black</code> reaction time variable in terms of the other variables in <code>targetingRaceDiff</code>.</p>
</section>
<section id="linear-modeling" class="level2">
<h2>Linear Modeling</h2>
<section id="basic-constant-fit" class="level3">
<h3>Basic (constant) fit</h3>
<p>We are looking for a linear regression model to understand the mean reaction time for black targets in terms of given inputs. Let us start with the simplest such model, often referred to as the “null model”, where we would like to predict the <code>Black</code> using no predictors at all. In that case all we can do is try to predict a single value, and then account for errors and variability around that value. Our model, as a formula, would look like this: <span class="math display">\[\textrm{Black} = \beta_0 + \epsilon\]</span> where the <span class="math inline">\(\beta_0\)</span> is a parameter we need to choose, and <span class="math inline">\(\epsilon\)</span> is the error we are making (different error for each point). The key question to address here is how to determine the “best value” for the parameter <span class="math inline">\(\beta_0\)</span>.</p>
<p>In linear regression, we choose the parameters so as to <em>minimize</em> the “residual sum of squares”, i.e. the sum of the squared residuals: <span class="math display">\[\textrm{RSS} = \sum \epsilon_i^2\]</span> In our case it can be seen easily that the choice of parameter value that minimizes this sum is the mean <span class="math inline">\(\beta_0 = \textrm{mean(y)}\)</span>. We can then use that to compute the RSS:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m &lt;-<span class="st"> </span>targetingRaceDiff<span class="op">$</span>Black <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">mean</span>()
m
rss &lt;-<span class="st"> </span><span class="kw">sum</span>((targetingRaceDiff<span class="op">$</span>Black <span class="op">-</span><span class="st"> </span>m)<span class="op">^</span><span class="dv">2</span>)
rss</code></pre></div>
<p>So we can see an average response time close to <span class="math inline">\(500\)</span> milliseconds, and that there is a total variability of <span class="math inline">\(410,973.6\)</span> to account for. Since we will often find ourselves computing the “sum of squared deviations” by subtracting the mean from a variable, then squaring, then summing all the values, let’s simplify matters by writing a small function that computes the squared deviations:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sq.devs &lt;-<span class="st"> </span><span class="cf">function</span> (x) { (x<span class="op">-</span><span class="kw">mean</span>(x))<span class="op">^</span><span class="dv">2</span> }
rss &lt;-<span class="st"> </span>targetingRaceDiff<span class="op">$</span>Black <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sq.devs</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sum</span>()</code></pre></div>
<p>We could get the same number using R’s modeling machinery:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit0 &lt;-<span class="st"> </span><span class="kw">lm</span>(Black<span class="op">~</span><span class="dv">1</span>, <span class="dt">data=</span>targetingRaceDiff)
<span class="kw">summary</span>(fit0)
<span class="kw">deviance</span>(fit0)    <span class="co"># Essentially the sum of squared deviations/residuals.</span></code></pre></div>
<p>The <code>1</code> on the right-hand-side of the model represents that we fit a constant model.</p>
<p>This null model is kind of a baseline against which we can compare our other models. This is essentially the simplest possible model; any other model should be doing better by comparison.</p>
<section id="optional-background-maximum-likelihood-estimation" class="level4">
<h4>Optional background: Maximum Likelihood Estimation</h4>
<p>There is a slightly different approach to the least squares method described above, and it proves to be easier to generalize to other settings. It roughly works as follows:</p>
<ul>
<li>We assume that the residuals are independent of each other and are all distributed identically, following a normal distribution centered at <span class="math inline">\(0\)</span> and with some standard deviation <span class="math inline">\(\sigma\)</span>. In that case the <span class="math inline">\(y\)</span> values follow a normal distribution centered at <span class="math inline">\(\beta_0\)</span>.</li>
<li>Therefore for each data point <span class="math inline">\(y_i\)</span> we can discuss the <em>likelihood/probability</em> that the <span class="math inline">\(y_i\)</span> would take this value, assuming the normal distribution and for a given value of <span class="math inline">\(\beta_0\)</span>.</li>
<li>We can then multiply all those likelihoods together, since the observations were independent, to get an <em>overall likelihood</em>. This is basically a number determining how how likely we are to observe this set of values given some fixed values for the parameters.</li>
<li>We now can choose the parameters that maximize this likelihood. This is known as the <strong>maximum likelihood estimate</strong>.</li>
</ul>
<p>It turns out that for linear regression, the solution to these two problems is exactly the same. So we can think of the coefficients provided by a linear regression fit in these two slightly different ways:</p>
<ul>
<li>They are those parameter values that minimize the overall error phrased as an RSS.</li>
<li>They are also those parameter values that maximize the likelihood of the values that we observed.</li>
</ul>
</section>
</section>
<section id="linear-fit-one-scalar-predictor" class="level3">
<h3>Linear fit, one scalar predictor</h3>
<p>Now we want to examine how the <code>Black</code> reaction time might be affected by other predictors. We will start by considering one such predictor, the <code>White</code> reaction time.</p>
<p>A graph is a good start. We did this earlier.</p>
<p>In a linear model we seek a formula that would describe in a linear way the response variable from the independent variables, accounting for a possible error. So the equation we are after would look like this: <span class="math display">\[\textrm{Black} = \beta_0 + \beta_1 \times \textrm{White} + \epsilon\]</span></p>
<p>The linear part, <span class="math inline">\(\beta_0 + \beta_1 \times \textrm{White}\)</span>, provides our <em>predicted value</em>, while the <span class="math inline">\(\epsilon\)</span> term indicates the error we are making (called the <em>residual</em>). In typical linear modeling there are numerous questions we like to ask:</p>
<ol type="1">
<li>Since we have many choices for the parameters <span class="math inline">\(\beta_i\)</span>, how do we define “the best choice”?</li>
<li>How can we assess whether the structure of the model is reasonable?</li>
<li>How do we determine how volatile our coefficients are to the variability in our data?</li>
<li>How can we use the model to make predictions, and what kind of error do we expect on those predictions?</li>
<li>How can we compare our model to other models?</li>
</ol>
<p>We essentially answered question 1 earlier. We saw there were two different ways to compute the best choice, and in the standard setting of a linear model they both result in the same estimates. Let us now construct a linear fit in R using the <code>White</code> predictor:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit1 &lt;-<span class="st"> </span><span class="kw">lm</span>(Black<span class="op">~</span>White, <span class="dt">data=</span>targetingRaceDiff)
<span class="kw">summary</span>(fit1)</code></pre></div>
<p>The output of this summary view tends to contain a lot of information. For now the one key piece of information is the fit coefficients, namely <span class="math inline">\(\beta_0 = 235.6828\)</span> and <span class="math inline">\(\beta_1=0.5203\)</span>. Therefore we are claiming that we have a model relationship that looks like so: <span class="math display">\[\textrm{Black} = 235.6828 + 0.5203 \times \textrm{White} + \epsilon\]</span> For instance, let us try to predict what the <code>Black</code> reaction time should be when <code>White</code> equals <span class="math inline">\(450\)</span>. We can do this either by direct computation using the above linear equation, or by using the <code>predict</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(fit1, <span class="kw">data.frame</span>(<span class="dt">White=</span><span class="dv">450</span>))
intercept &lt;-<span class="st"> </span><span class="kw">coef</span>(fit1)[<span class="dv">1</span>]    <span class="co"># 235.6828</span>
slope &lt;-<span class="st"> </span><span class="kw">coef</span>(fit1)[<span class="dv">2</span>]        <span class="co"># 0.5203</span>
intercept <span class="op">+</span><span class="st"> </span>slope <span class="op">*</span><span class="st"> </span><span class="dv">450</span>       <span class="co"># prediction at White = 450</span></code></pre></div>
<p>In general, we can interpret the slope of <span class="math inline">\(0.52\)</span> as telling us that the reaction time for Black targets changes at half the rate as the reaction time for White targets.</p>
<p>One of the questions we’ll want to answer is how reliable this prediction is; we will return to that later.</p>
<p>For now let us discuss how to assess how good of a fit this is. There are two questions to consider:</p>
<ol type="1">
<li>Is this model significantly better than the null model that uses no predictors?</li>
<li>Do the individual predictors that are used have a considerable contribution to the model? In this case as there is only one predictor, this is the same question as 1.</li>
</ol>
<p>To answer question 1, a first place to look is the F-statistic and its p-value. In our case that is <span class="math inline">\(F=92.18\)</span> on <span class="math inline">\(1\)</span> and <span class="math inline">\(189\)</span> degrees of freedom, with a p-value less than <code>2.2e-16</code>, indicating that the overall model that includes the <code>White</code> variable is an improvement over the null model.</p>
<p>To answer question 2, we would typically look at the t-value for the coefficient, and its p-value, which in this case happens to be the same.</p>
<p>We can get all the predicted values and all the residuals by simply doing respectively:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(fit1)
<span class="kw">resid</span>(fit1)</code></pre></div>
<p>Perhaps the most useful diagnostic for assessing a model fit is to look at the fit’s diagnostic plots. The following code automatically creates four different diagnostic plots:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(fit1)</code></pre></div>
<p>The first of these plots is a typical <em>residual plot</em>, showing the residuals vs the fitted values. In this plot we typically look for two things: non-linear patterns might be an indication that we need a different form for our model (for example perhaps a quadratic term for <code>White</code>); also we hope to see consistent variance in the residuals across the range of the fitted values (homoskedasticity). Looking at the residual plot for our <code>fit1</code> raises no concerns.</p>
<p>The second plot is a normal quantile plot of the standardized residuals. This helps us assess the assumption that the errors should be normally distributed (which translates to a normal distribution for the <em>standardized</em> residuals, which we will discuss more later). The normal quantile plot for our fit shows a linear pattern, which suggests that our standardized residuals are in fact normally distributed.</p>
<p>The third plot gives us an alternative method for assessing the assumption of homoskedasticity.</p>
<p>The fourth plot helps us identify influential observations, which may be unduly influencing the model fit. The x-axis is a point’s <em>leverage</em>, which is a measure of how far that data point is from the rest of the data. Such points tend to pull the fit towards them, hence the term <code>leverage</code>. The y-axis is the point’s standardized residual. We are typically interested in points that both influential and have large residuals, so the product of the two values, typically described by the so-called “Cook’s distance”. The red dashed lines indicate thresholds for the Cook’s distance, and values beyond those thresholds warrant a closer look.</p>
<section id="standardized-and-studentized-residuals" class="level4">
<h4>Standardized and Studentized residuals</h4>
<p>In a linear model we make an assumption that our errors are normally distributed. When trying to assess that assumption, we have at our disposal not the errors themselves but the estimates of those errors in the form of the residuals. These estimates turn out to not be exactly independent and normally distributed. But the corresponding <em>standardized residuals</em> should be normally distributed. The <code>rstandard</code> method in R computes these standardized residuals:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">rstandard</span>(fit1)</code></pre></div>
<p>In order to test for normality, it is best to use these standardized residuals, though typically they won’t be all that different from the raw residuals. These are also often called <em>internally studentized residuals</em>.</p>
<p>You may also run into another kind of residual, called <em>(externally) studentized residual</em>. These can be computed in R with the <code>rstudent</code> method.</p>
</section>
<section id="prediction-and-estimation" class="level4">
<h4>Prediction and Estimation</h4>
<p>When trying to use a model to make a prediction for the <span class="math inline">\(y\)</span> value at a particular value <span class="math inline">\(x\)</span>, there are two slightly different values that we might be trying to predict:</p>
<ol type="1">
<li>The average of all the possible <span class="math inline">\(y\)</span> values for that particular <span class="math inline">\(x\)</span> (i.e. a <em>predicted mean response</em>).</li>
<li>An actual possible <span class="math inline">\(y\)</span> value for that particular <span class="math inline">\(x\)</span> (i.e. a <em>prediction of a future observation</em>).</li>
</ol>
<p>Even though in both cases the estimate is the same, namely the result of plugging in the <span class="math inline">\(x\)</span> value to the formula of the estimates, the two cases differ considerably in the estimation of the standard error and consequently the construction of confidence intervals.</p>
<p>For the former, we simply need to account for the variability in the estimation of the parameters <span class="math inline">\(\beta\)</span>. This is essentially the standard deviation <span class="math inline">\(\sigma\)</span> of the residuals suitably scaled to account for the <span class="math inline">\(x\)</span> value. The resulting intervals are called <em>confidence intervals</em>.</p>
<p>For the latter, we have to account for the extra variability <span class="math inline">\(\sigma\)</span> due to the possible additive <em>error</em> term at that particular <span class="math inline">\(x\)</span>. The combination of the two independent variabilities is the desired variability. The resulting intervals are called <em>prediction intervals</em>.</p>
<p>In R, the <code>predict</code> method will provide us with confidence and prediction intervals:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(fit1, <span class="kw">list</span>(<span class="dt">White=</span><span class="dv">450</span>), <span class="dt">interval=</span><span class="st">&quot;confidence&quot;</span>)
<span class="kw">predict</span>(fit1, <span class="kw">list</span>(<span class="dt">White=</span><span class="dv">450</span>), <span class="dt">interval=</span><span class="st">&quot;prediction&quot;</span>)</code></pre></div>
<p>As anticipated, for each <span class="math inline">\(x\)</span>, the prediction interval at <span class="math inline">\(x\)</span> is wider than the confidence interval for the <span class="math inline">\(y\)</span> mean at that <span class="math inline">\(x\)</span>.</p>
</section>
</section>
<section id="linear-fit-one-factor" class="level3">
<h3>Linear fit, one factor</h3>
<p>Let us now consider a factor variable and look at its effect on <code>Black</code>. A factor variable can make a single prediction for each factor level, and so it may be able to do better than the initial null model, which was only making a single prediction. Let’s consider the <code>weapon</code> variable, which refers to whether the target was armed or unarmed. We might expect faster reaction times if the target is armed. We start with a plot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(targetingRaceDiff) <span class="op">+</span>
<span class="st">    </span><span class="kw">aes</span>(<span class="dt">x=</span>weapon, <span class="dt">y=</span>Black) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_boxplot</span>()</code></pre></div>
<p>We could also do a t-test:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(<span class="op">~</span>Black<span class="op">|</span>weapon, <span class="dt">data=</span>targetingRaceDiff)</code></pre></div>
<p>This is treated as independent-samples test, which is clearly not the right thing in our case since we have matched pairs. In order to do that matched pairs, test, we would need to spread the data first:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">targetingRaceDiff <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(<span class="op">-</span>White) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">spread</span>(weapon, Black) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">with</span>(<span class="kw">t.test</span>(Armed, Unarmed, <span class="dt">paired=</span><span class="ot">TRUE</span>))</code></pre></div>
<p>We could also use <code>dplyr</code> to compute some numerical summaries for each group:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">targetingRaceDiff <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">group_by</span>(weapon) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">summarize</span>(<span class="dt">mean =</span> <span class="kw">mean</span>(Black),
              <span class="dt">sd  =</span> <span class="kw">sd</span>(Black),
              <span class="dt">n   =</span> <span class="kw">n</span>(),
              <span class="dt">se  =</span> <span class="kw">sd</span>(Black)<span class="op">/</span><span class="kw">sqrt</span>(n))</code></pre></div>
<p>Let’s take a look at a model fit:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">fit2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Black <span class="op">~</span><span class="st"> </span>weapon, <span class="dt">data=</span>targetingRaceDiff)
<span class="kw">summary</span>(fit2)
<span class="kw">coef</span>(fit2)</code></pre></div>
<p>We can see that the model output has treated the “Armed” case as a baseline, and the intercept represents the mean/predicted value for <code>Black</code> for those subjects in the “Armed” case. The effect for the “Unarmed” case is then considered as an additive factor to that. So the predicted value for the “Armed” case would be <code>coef(fit2)[1] = 485.5178</code> and the predicted value for the “Unarmed” case would be <code>coef(fit2)[1] + coef(fit2)[2] = 505.4491</code>. These are of course the same as the mean values we saw with dplyr.</p>
<p>Notice the p-value of <span class="math inline">\(0.00284\)</span> which appears in two places. It is the P-value for an F test that measures if our model is better than the null model, i.e. than the case where the values for armed and unarmed were the same. Or it can be thought of as the P-value for a t test on whether the coefficient for the term <code>weaponUnarmed</code> is non-zero. We discuss these tests in more detail in the next section.</p>
</section>
<section id="the-f-statistic-optional" class="level3">
<h3>The F statistic (optional)</h3>
<p>In general, if we have two models <span class="math inline">\(M_1\)</span> and <span class="math inline">\(M_2\)</span> with <span class="math inline">\(M_2\)</span> being the an extension of <span class="math inline">\(M_1\)</span>, and with degrees of freedom <span class="math inline">\(df_1\)</span> and <span class="math inline">\(df_2\)</span> respectively, then we can consider the difference between the residual sums of squares of the two models scaled by the difference in the degrees of freedom, and divide that by the scaled residual for the larger model: <span class="math display">\[F=\frac{(\textrm{RSS}(M_1) - \textrm{RSS}(M_2) )/(df_1-df_2)}{\textrm{RSS}(M_2)/df_2}\]</span> Assuming that the larger model <span class="math inline">\(M_2\)</span> does not provide any improvement over the smaller model, then this number <span class="math inline">\(F\)</span> follows an <span class="math inline">\(F_{df_1-df_2, df_2}\)</span> distribution.</p>
<p>As an example in our case, we have our larger model <span class="math inline">\(M_2\)</span> that uses <code>weapon</code> in addition to a constant to determine <code>Black</code>, and we want to compare it to the null model, which uses just the constant. We have <span class="math inline">\(df_2=n-2\)</span> and <span class="math inline">\(df_1=n-1\)</span>. We can directly compute the sums of squared residuals of the two models:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">rss1 &lt;-<span class="st"> </span><span class="kw">residuals</span>(fit0) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sq.devs</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sum</span>()
rss2 &lt;-<span class="st"> </span><span class="kw">residuals</span>(fit2) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sq.devs</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sum</span>()
n &lt;-<span class="st"> </span><span class="kw">nrow</span>(targetingRaceDiff)
df1 &lt;-<span class="st"> </span>n<span class="op">-</span><span class="dv">1</span>
df2 &lt;-<span class="st"> </span>n<span class="op">-</span><span class="dv">2</span>
fstat &lt;-<span class="st"> </span>((rss1<span class="op">-</span>rss2)<span class="op">/</span>(df1<span class="op">-</span>df2)) <span class="op">/</span><span class="st"> </span>(rss2<span class="op">/</span>df2)
fstat; df1<span class="op">-</span>df2; df2
<span class="kw">pf</span>(fstat, df1<span class="op">-</span>df2, df2, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)</code></pre></div>
<p>The last line tells R to compute the upper-tail probability for the value <code>fstat</code> in an F distribution with <code>df1-df2</code> and <code>df2</code> degrees of freedom. There are a number of functions like <code>pf</code> for all kinds of distributions. You can read more about them via <code>?Distributions</code>.</p>
<p>You may be familiar with these computations under a different terminology. The denominator can be interpreted as the <strong>within-groups variability</strong>, while the numerator can be interpreted as the <strong>between-groups variability</strong>. Let’s check this in our instance. We can define the between-groups variability as follows: Data points form two groups based on their <code>weapon</code> value. For each point we consider the difference between the mean of the point’s group vs the overall mean, then we look at the sum of squares of these differences. In R this would be:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">between.groups &lt;-<span class="st"> </span>targetingRaceDiff <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">totalMean=</span><span class="kw">mean</span>(Black)) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">group_by</span>(weapon) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">groupMean=</span><span class="kw">mean</span>(Black)) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">summarize</span>(<span class="dt">between.groups=</span><span class="kw">sum</span>((groupMean<span class="op">-</span>totalMean)<span class="op">^</span><span class="dv">2</span>))
within.groups &lt;-<span class="st"> </span>targetingRaceDiff <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">group_by</span>(weapon) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">sqdevs =</span> <span class="kw">sq.devs</span>(Black)) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">ungroup</span>() <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">summarize</span>(<span class="dt">within.groups=</span><span class="kw">sum</span>(sqdevs))

between.groups; within.groups
(between.groups <span class="op">/</span><span class="st"> </span>(df1 <span class="op">-</span><span class="st"> </span>df2)) <span class="op">/</span><span class="st"> </span>(within.groups <span class="op">/</span><span class="st"> </span>df2)</code></pre></div>
<p>The idea of the test is that if the model with <code>weapon</code> is not a considerable improvement over the model without <code>weapon</code>, then the between-groups variability will be small compared to the within-groups variability.</p>
<p>Of course, instead of doing all this by hand, the <code>summary</code> method for the fit does the work for us:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(fit2)</code></pre></div>
<p>We can also see the same computation in the <code>anova</code> function, which compares two models via the method described above:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(fit0, fit2)</code></pre></div>
<p>Analogous tests can be performed on the coefficients of the fit directly, using the <span class="math inline">\(t\)</span> distribution. Testing for a coefficient equaling 0 is equivalent to an F-test where we compare the full model with the smaller model without that coefficient. In the cases we have seen so far, this coincides with the test of the full model against the null model.</p>
</section>
<section id="rs-distribution-functions" class="level3">
<h3>R’s distribution functions</h3>
<p>While most of the time you would rely on the outputs of the <code>summary</code> and <code>anova</code> functions for your p-value computations, you may on occasion find it useful to compute some of these directly. R offers us a whole bevy of distribution functions to help with that, and you can find more about them by typing <code>?Distributions</code> in the console.</p>
<p>R knows about over 20 different distributions, and of each distribution it provides us with 4 functions:</p>
<ul>
<li><code>r...</code> can be used to produce random values following a specific distribution. For example `rnorm(1000, mean=3, sd=1) would give us 1000 values drawn from a normal distribution with a mean of 3 and standard deviation of 1.</li>
<li><code>d...</code> is the actual density function for the distribution, and is not particularly useful.</li>
<li><code>p...</code> returns probability values for a given x value. For example <code>pf(9.143, df1=1, df2=189, lower.tail=FALSE)</code> is the p-value 0.00284 for an F-test with an F value of 9.143 and degrees of freedom 1 and 189.</li>
<li><code>q...</code> returns the quantile for a specified probability. For example to find the 90th percentile on a t-distribution with 30 degrees of freedom, we would do <code>qt(0.9, df=30, lower.tail=TRUE)</code>.</li>
</ul>
</section>
<section id="factors-with-multiple-levels" class="level3">
<h3>Factors with multiple levels</h3>
<p>Let us briefly discuss a case with a factor that has more than two levels. Such a factor will add one more parameter, hence one less degree of freedom. This presents an opportunity to discuss how factor levels may be coded and their various effects, and it will also be an opportunity to demonstrate the package <code>GGally</code> for producing some interesting plots.</p>
<p>We will use the <code>iris</code> data set, which contains measurements on the petal and sepal lengths and widths of 150 different iris plants, from three different species.</p>
<p>The <code>GGally</code> package offers us a nice visualization of the whole dataset. You will first need to install the package, via “Install” button in the <code>Packages</code> pane. Make sure you spell it correctly, with two capital Gs. After you have installed it, the following code should work:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(GGally)
iris <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(Species) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">summarize</span>(<span class="dt">mean=</span><span class="kw">mean</span>(Petal.Width), <span class="dt">sd=</span><span class="kw">sd</span>(Petal.Width), <span class="dt">n=</span><span class="kw">n</span>())
<span class="kw">ggpairs</span>(iris, <span class="kw">aes</span>(<span class="dt">color =</span> Species), <span class="dt">progress=</span><span class="ot">FALSE</span>)</code></pre></div>
<p>Looking at this plot, we can see that each species distinguishes itself in some way. For example the <code>setosa</code> irises have unusually small petal lengths and widths, while the <code>virginica</code> irises tend to have relatively large petal lengths and widths.</p>
<p>For practice, let us set up a model to fit the petal width against the species:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">irisFit &lt;-<span class="st"> </span><span class="kw">lm</span>(Petal.Width<span class="op">~</span>Species, <span class="dt">data=</span>iris)
<span class="kw">summary</span>(irisFit)</code></pre></div>
<p>We see in this example that R has set up the <code>setosa</code> species as a baseline, and has introduced two additive coefficients, one for <code>versicolor</code> and one for <code>virginica</code>. So we can see that the average petal width for setosas is <span class="math inline">\(0.246\)</span>, while for versicolors it would be <span class="math inline">\(0.246+1.08=1.326\)</span>, and for virginicas it would be <span class="math inline">\(0.246+1.78=2.026\)</span>.</p>
<p>We can also see a very small p-value for the F statistic, meaning that the species variable definitely has a significant effect. We also notice the t-tests for the two terms against the base point of setosa.</p>
<p>What we see in use here is what is known as <strong>treatment contrast</strong>, with one baseline entry, typically representing the control group, and a 0/1 coding for each of the subsequent levels. There are many available contrast coding systems, and you can search the documentation to learn more.</p>
<p>We will use a package called <code>broom</code> to do some work with the model outputs and produce suitable plots.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(broom)
<span class="kw">tidy</span>(irisFit, <span class="dt">conf.int=</span><span class="ot">TRUE</span>)
<span class="kw">glance</span>(irisFit)
<span class="kw">augment</span>(irisFit)
<span class="kw">augment</span>(irisFit) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">    </span><span class="kw">aes</span>(<span class="dt">x=</span>Species, <span class="dt">color=</span>Species, <span class="dt">y=</span>.resid) <span class="op">+</span>
<span class="st">    </span><span class="kw">geom_boxplot</span>()</code></pre></div>
<p>Note that this last graph suggests a violation of the homoscedasticity assumption.</p>
</section>
<section id="comparing-level-differences" class="level3">
<h3>Comparing level differences</h3>
<p>We can get pairwise comparisons between the levels by using Tukey’s HSD (honest significant difference) test:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">TukeyHSD</span>(irisFit)
<span class="kw">TukeyHSD</span>(irisFit) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">plot</span>()</code></pre></div>
<p>This test is a single-step-multiple-comparison procedure that constructs confidence intervals for all pairwise differences between the factor levels, using a <em>studentized range distribution</em>. You can read more about this test in the documentation and the internet.</p>
</section>
</section>
</section>
</body>
</html>
